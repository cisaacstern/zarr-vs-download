{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pangeo Forge\n",
    "## Transform large data archives into analysis-ready, cloud-optimized (ARCO) data stores\n",
    "\n",
    "Presentation prepared by Charles Stern ([@cisaacstern](http://github.com/cisaacstern)), Data Infrastructure Engineer, Lamont-Doherty Earth Observatory (LDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from intake import open_catalog\n",
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15,10)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll start and connect to this cluster ahead of time, but time it, so you'll know how long it took!\n",
    "start = time.time()\n",
    "\n",
    "gateway = Gateway()\n",
    "cluster = gateway.new_cluster()\n",
    "cluster.adapt(minimum=1, maximum=20)\n",
    "client = Client(cluster)\n",
    "print(f\"Connected to Dask client in {round(time.time()-start, 2)} seconds\")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMEMS sea surface altimetry data\n",
    "\n",
    "For this example we will use [gridded sea-surface altimetry data](http://marine.copernicus.eu/services-portfolio/access-to-products/?option=com_csw&view=details&product_id=SEALEVEL_GLO_PHY_L4_REP_OBSERVATIONS_008_047) from The Copernicus Marine Environment. This is a widely used dataset in physical oceanography and climate. We will cover how to extracted the dataset from Copernicus store it in google cloud storage in [xarray-zarr](http://xarray.pydata.org/en/latest/io.html#zarr) format with the aid of [pangeo-forge-recipes](https://github.com/pangeo-forge/pangeo-forge-recipes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familiarize ourselves with the `ftp` index\n",
    "\n",
    "Opening `cmems_index/index.html` gives us a top level view of the `ftp` server directory structure, which looks something like this:\n",
    "\n",
    "```\n",
    "ftp://my.cmems-du.eu:21/Core/SEALEVEL_GLO_PHY_L4_REP_OBSERVATIONS_008_047/dataset-duacs-rep-global-merged-allsat-phy-l4/\n",
    " ├──/1993\n",
    " │   ├──/01\n",
    " │   │   ├── dt_global_allsat_phy_l4_19930101_20190101.nc  (7789577 bytes)\n",
    " │   │  ...\n",
    " │   │   └── dt_global_allsat_phy_l4_19930131_20190101.nc  (7853172 bytes)\n",
    " │  ...\n",
    " │   └──/12\n",
    "...\n",
    " └──/2020\n",
    "```\n",
    "\n",
    "With a little help from `pandas`, we can see there's a total of 8901 files in our target range (January 1993 - mid March 2017):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range(start=\"1993-01-01\", end=\"2017-05-15\")\n",
    "len(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start downloading (hint: this is going to take a while!)\n",
    "\n",
    "The script `wget_cmems.py` will take care of this for us. Let's get it started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A better way: Pangeo Forge\n",
    "\n",
    "`pangeo-forge-recipes` provides logic for transforming all of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's a `recipe`?\n",
    "A `recipe` is a Python file which can \"see\" all of the source files, and also knows how to logically arrange them into a cohesive dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmems_recipe import recipe\n",
    "\n",
    "for time_index, url in recipe.file_pattern.items():\n",
    "    if time_index[0] < 3 or time_index[0] > 8897:\n",
    "        # note the '\\033[...' are to color the output\n",
    "        print(f\"\\033[1;32m{time_index}\", f\"\\033[0m{url}\")\n",
    "    elif time_index[0] == 8897:\n",
    "        print(\"...\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are exactly the same files as we've begun downloading with `wget_cmems.py`, with the addition of these \"alignment keys\" (printed above in green) which allow `pangeo-forge-recipes` to not just *access* the source files, but also to **assemble** them into a cloud-optimized zarr store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataset\n",
    "\n",
    "> In order to build a zarr store from the source files, the recipe must be \"executed,\" which can happen in a number of ways. The most basic requirements for execution are a Python installation and an internet connection, but we're working on automating this process as we speak. \n",
    "\n",
    "I've already completed this build using `cmems_recipe.py`. From this point forward, getting up and running with the data is nearly instantaneous. There are a few options for loading the dataset from the zarr store, but using an Intake catalog is one of the easiest. Note that these very large datasets initialize nearly instantaneously, and we can see the full list of variables and coordinates, including metadata for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = open_catalog(\"catalog.yaml\")\n",
    "\n",
    "for source in [\"full_altimetry\", \"anomalies_only\"]:\n",
    "    ds = cat[source].to_dask()\n",
    "    bold, reset = '\\033[1m', '\\033[0m'\n",
    "    print(f\"{bold}'{source}' is {round(ds.nbytes/1e9, 2)} GBs {reset}and contains {ds.data_vars} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visually Examine Some of the Data\n",
    "\n",
    "Let's do a sanity check that the data looks reasonable. Here we use the [hvplot](https://hvplot.holoviz.org/index.html) interactive plotting library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sla.hvplot.image('longitude', 'latitude',\n",
    "                    rasterize=True, dynamic=True, width=800, height=450,\n",
    "                    widget_type='scrubber', widget_location='bottom', cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example calculation: timeseries of Global Mean Sea Level\n",
    "\n",
    "Here we make a simple yet fundamental calculation: the rate of increase of global mean sea level over the observational period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of GB involved in the reduction\n",
    "ds.sla.nbytes/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the computationally intensive step\n",
    "sla_timeseries = ds.sla.mean(dim=('latitude', 'longitude')).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sla_timeseries.plot(label='full data')\n",
    "sla_timeseries.rolling(time=365, center=True).mean().plot(label='rolling annual mean')\n",
    "plt.ylabel('Sea Level Anomaly [m]')\n",
    "plt.title('Global Mean Sea Level')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pause for discussion:\n",
    "\n",
    "1. Wait a second, remember the download we started with `wget_cmems.py`?\n",
    "   - We just did a lot of work analyzing a 73 GB subset of a 500+ GB dataset.\n",
    "   - How much of that same dataset has been downloaded over these last few minutes?\n",
    "   - Let's see ...\n",
    "2. How do I make my own recipe?\n",
    "   - Check out the docs: https://pangeo-forge.readthedocs.io/en/latest/\n",
    "   - Reach out by opening an Issue here: https://github.com/pangeo-forge/staged-recipes/issues.\n",
    "   - One prerequisite is that your dataset is in a format that can be opened with `xarray` (netCDF, etc.).\n",
    "3. What's next for `pangeo-forge-recipes`?\n",
    "   - Automated recipe execution in \"Bakeries\"\n",
    "   - Cataloging with STAC\n",
    "4. Your questions!\n",
    "   - Reach out on GitHub if you think of them later :-)\n",
    "\n",
    "Feel free to complete the additional computations and plots below on your own time. They're pretty interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional computations: Hovöller diagram & standard deviation\n",
    "\n",
    "Here are some additional computations to run on the Dask cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand how the sea level rise is distributed in latitude, we can make a sort of [Hovmöller diagram](https://en.wikipedia.org/wiki/Hovm%C3%B6ller_diagram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another distributed computation\n",
    "sla_hov = ds.sla.mean(dim='longitude').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "sla_hov.name = 'Sea Level Anomaly [m]'\n",
    "sla_hov.transpose().plot(vmax=0.2, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most sea level rise is actually in the Southern Hemisphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sea Level Variability\n",
    "\n",
    "We can examine the natural variability in sea level by looking at its standard deviation in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one last computation\n",
    "sla_std = ds.sla.std(dim='time').load()\n",
    "sla_std.name = 'Sea Level Variability [m]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sla_std.plot()\n",
    "_ = plt.title('Sea Level Variability')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}